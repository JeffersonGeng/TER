{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Introduction \n","Autoencoder are special type of deep learning architecture that consist of two networks encoder and decoder.\n","The encoder, through a series of CNN and downsampling, learns a reduced dimensional representation of the input data while decoder  through the use of CNN and upsampling, attempts to regenerate the data from the these representations. A well-trained decoder is able to regenerated data that is identical or as close as possible to the original input data.\n","Autoencoder are generally used for anamoly detection, denoising image, colorizing the images. Here, i am going to colorize the landscape images using autoencoder."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src = 'https://miro.medium.com/max/600/1*nqzWupxC60iAH2dYrFT78Q.png' >"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Image Colorization\n","Image colorization using different softwares require large amount of human effort, time and skill.But special type of deep learning architecture called autoencoder has made this task quiet easy. Automatic image colorization often involves the use of a class of convolutional neural networks (CNN) called autoencoders. These neural networks are able to distill the salient features of an image, and then regenerate the image based on these learned features. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<img src = \"https://tinyclouds.org/colorize/best/6.jpg\">"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Import necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import keras\n","import cv2\n","from keras.layers import MaxPool2D,Conv2D,UpSampling2D,Input,Dropout\n","from keras.models import Sequential\n","from keras.utils import img_to_array, plot_model\n","from keras.utils.vis_utils import plot_model\n","import os\n","from tqdm import tqdm\n","import re\n","import matplotlib.pyplot as plt\n","import gc"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Getting landscape image data,resizing them and appending in array\n","To get the image in sorted order i have defined the function sorted_alphanumeric. Here, I have used open cv library to read and resize images. Finally images are normalized and are converted to array and are appended in empty list"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def sorted_alphanumeric(data):  \n","    convert = lambda text: int(text) if text.isdigit() else text.lower()\n","    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n","    return sorted(data,key = alphanum_key)\n","# defining the size of the image\n","SIZE = 256\n","color_img = []\n","gray_img = []\n","path = 'E:/TER/LandscapeDataResize/color'\n","files = os.listdir(path)\n","files = sorted_alphanumeric(files)\n","for i in tqdm(files):    \n","        if i == '7200.jpg':\n","            break\n","        else:    \n","            img = cv2.imread(path + '/'+i,1)\n","            # open cv reads images in BGR format so we have to convert it to LAB\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)\n","            #resizing image\n","            #分离图像通道\n","            l, a, b = cv2.split(img)\n","            xx=b\n","            #归一化\n","            l = (l/ 255.0)\n","            a = (a/ 255.0)\n","            b = (b/ 255.0)\n","            #合并通道\n","            img = cv2.merge([l, a, b])\n","            color_img.append(img_to_array(img))\n","            gray_img.append(img_to_array(l))\n","gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Plotting Color image and it's corresponding grayscale image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# defining function to plot images pair\n","def plot_images(color,grayscale):\n","    plt.figure(figsize = (10,10))\n","    plt.subplot(1,2,1)\n","    plt.title('color')\n","    l, a, b = cv2.split(np.array(color))\n","    #归一化\n","    l = (l.astype('float32')* 100)\n","    a = (a.astype('float32')* 255.0)-128\n","    b = (b.astype('float32')* 255.0)-128\n","    #合并通道\n","    color = cv2.merge([l, a, b])\n","    plt.imshow(cv2.cvtColor(color, cv2.COLOR_Lab2RGB))\n","    plt.subplot(1,2,2)\n","    plt.title('gray')\n","    plt.imshow(grayscale, cmap = 'gray')\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Plotting image pair**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i in range(3,10):\n","     plot_images(color_img[i],gray_img[i])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Slicing and reshaping\n","Out of 5000 images I have sliced them to two part. train images consist 4000 images  while test images contains 1000 images.\n","After slicing the image array, I reshaped them so that images can be fed directly into our encoder network"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_gray_image = gray_img[:4000]\n","train_color_image = color_img[:4000]\n","\n","test_gray_image = gray_img[4000:]\n","test_color_image = color_img[4000:]\n","del gray_img\n","del color_img\n","gc.collect()\n","# reshaping\n","train_g = np.reshape(train_gray_image,(len(train_gray_image),SIZE,SIZE,1))\n","train_c = np.reshape(train_color_image, (len(train_color_image),SIZE,SIZE,3))\n","print('Train color image shape:',train_c.shape)\n","\n","\n","test_gray_image = np.reshape(test_gray_image,(len(test_gray_image),SIZE,SIZE,1))\n","test_color_image = np.reshape(test_color_image, (len(test_color_image),SIZE,SIZE,3))\n","print('Test color image shape',test_color_image.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Defining our model\n","Encoder layer of our model consist blocks of Convolution layer with different number of kernel and kernel_size. Here, Convolution is used for downsampling.\n","Similary, Decoder layer of our model consist of  transpose convolution layer with different kernel size. Here, Decoder layer upsample image downsampled by encoder.\n","Since there is feature loss between the encoder and decoder layers so inorder to prevent feature loss i have concatenate corresponding encoder and decoder layers. Check U_Net architecture for better understanding......"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def downsample(filters, size, apply_batchnorm=True):\n","  \n","  result = tf.keras.Sequential()\n","  result.add(\n","      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                             kernel_initializer='he_normal', use_bias=False))\n","\n","  if apply_batchnorm:\n","    result.add(tf.keras.layers.BatchNormalization())\n","\n","  result.add(tf.keras.layers.LeakyReLU())\n","\n","  return result\n","\n","def upsample(filters, size, apply_dropout=False):\n","  \n","\n","  result = tf.keras.Sequential()\n","  result.add(\n","    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                    padding='same',\n","                                    kernel_initializer='he_normal',\n","                                    use_bias=False))\n","\n","  result.add(tf.keras.layers.BatchNormalization())\n","\n","  if apply_dropout:\n","      result.add(tf.keras.layers.Dropout(0.5))\n","\n","  result.add(tf.keras.layers.ReLU())\n","\n","  return result  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras import layers\n","def down(filters , kernel_size, apply_batch_normalization = True):\n","    downsample = tf.keras.models.Sequential()\n","    downsample.add(layers.Conv2D(filters,kernel_size,padding = 'same', strides = 2))\n","    if apply_batch_normalization:\n","        downsample.add(layers.BatchNormalization())\n","    downsample.add(keras.layers.LeakyReLU())\n","    return downsample\n","\n","\n","def up(filters, kernel_size, dropout = False):\n","    upsample = tf.keras.models.Sequential()\n","    upsample.add(layers.Conv2DTranspose(filters, kernel_size,padding = 'same', strides = 2))\n","    if dropout:\n","        upsample.dropout(0.2)\n","    upsample.add(keras.layers.LeakyReLU())\n","    return upsample"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def autoencoder_lab():\n","    inputs = tf.keras.layers.Input(shape=[256,256,1])\n","\n","    down_stack = [\n","        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n","        downsample(128, 4), # (bs, 64, 64, 128)\n","        downsample(256, 4), # (bs, 32, 32, 256)\n","        downsample(512, 4), # (bs, 16, 16, 512)\n","        downsample(512, 4), # (bs, 8, 8, 512)\n","        downsample(512, 4), # (bs, 4, 4, 512)\n","        downsample(512, 4), # (bs, 2, 2, 512)\n","        downsample(512, 4), # (bs, 1, 1, 512)\n","    ]\n","\n","    up_stack = [\n","        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n","        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n","        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n","        upsample(512, 4), # (bs, 16, 16, 1024)\n","        upsample(256, 4), # (bs, 32, 32, 512)\n","        upsample(128, 4), # (bs, 64, 64, 256)\n","        upsample(64, 4), # (bs, 128, 128, 128)\n","    ]\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    last = tf.keras.layers.Conv2DTranspose(3, 4,\n","                                        strides=2,\n","                                        padding='same',\n","                                        kernel_initializer=initializer,\n","                                        activation='tanh') # (bs, 256, 256, 3)\n","\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = tf.keras.layers.Concatenate()([x, skip])\n","\n","    x = last(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["autoencoder_lab =autoencoder_lab()\n","autoencoder_lab.summary()\n","plot_model(autoencoder_lab, to_file='autoencoder_lab.png', show_shapes=True, show_layer_names=True,rankdir='TB', dpi=320)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Fitting our model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["autoencoder_lab.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mean_absolute_error',\n","              metrics = ['acc'])\n","results=autoencoder_lab.fit(train_g, train_c, epochs = 100, batch_size = 16, verbose = 1)\n","autoencoder_lab.save('E:/TER/model/autoencoder_lab.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["autoencoder_lab.evaluate(test_gray_image,test_color_image)\n","plt.plot(results.history['loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.savefig('./loss_autoencoder_lab.png')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# plotting colorized image along with grayscale and color image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# defining function to plot images pair\n","def plot_images(color,grayscale,predicted,n):\n","    M = tf.keras.losses.MeanSquaredError()\n","    MSE = M(color, predicted)\n","    plt.figure(figsize=(15,5))\n","    l, a, b = cv2.split(np.array(color))\n","    #归一化\n","    l = (l.astype('float32')* 100)\n","    a = (a.astype('float32')* 255.0)-128\n","    b = (b.astype('float32')* 255.0)-128\n","    #合并通道\n","    target_image = cv2.merge([l, a, b])  \n","    l, a, b = cv2.split(np.array(predicted))\n","    #归一化\n","    l = (l.astype('float32')* 100)\n","    a = (a.astype('float32')* 255.0)-128\n","    b = (b.astype('float32')* 255.0)-128\n","    #合并通道\n","    prediction_image = cv2.merge([l, a, b])\n","    target_image = cv2.cvtColor(target_image, cv2.COLOR_LAB2RGB)\n","    prediction_image = cv2.cvtColor(prediction_image, cv2.COLOR_LAB2RGB)    \n","    plt.imsave('E:/TER/Result_autoencoder_LandscapeDataResize_LAB/output/'+str(n)+'.png', prediction_image)\n","    display_list = [grayscale, target_image, prediction_image]\n","    title = ['Input Image', 'Ground Truth', 'Predicted Image in LAB']\n","    for i in range(3):\n","        plt.subplot(1, 3, i+1)\n","        plt.title(title[i])\n","        # getting the pixel values between [0, 1] to plot it.\n","        plt.imshow(display_list[i],cmap='gray')\n","        plt.axis('off')\n","    plt.suptitle(\"MSE: \"+str(MSE.numpy()))\n","    plt.savefig('E:/TER/Result_autoencoder_LandscapeDataResize_LAB/plot/'+str(n)+'.png')\n","    plt.show()\n","\n","if not os.path.exists('E:/TER/Result_autoencoder_LandscapeDataResize_LAB/output/'):\n","    os.makedirs('E:/TER/Result_autoencoder_LandscapeDataResize_LAB/output/')\n","if not os.path.exists('E:/TER/Result_autoencoder_LandscapeDataResize_LAB/plot/'):\n","    os.makedirs('E:/TER/Result_autoencoder_LandscapeDataResize_LAB/plot/')\n","\n","for i in range(319):\n","    predicted = np.clip(autoencoder_lab.predict(test_gray_image[i].reshape(1,SIZE, SIZE,1)),0.0,1.0).reshape(SIZE, SIZE,3)\n","    plot_images(test_color_image[i],test_gray_image[i],predicted,i)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Thanks for your visit.\n","## Any suggestions to improve this model is highly appreciated.\n","# Feel free to  comment"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
